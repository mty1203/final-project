# TSV + Probe 生成 Steering 实验诊断报告

## 问题现象

在运行 `steer_with_probe.py` 时，无论 baseline (alpha=0) 还是 steering (alpha>0)，生成的文本都是重复的 "NASL NASL NASL..."。

## 根本原因

经过详细调试，发现了两个关键问题：

### 问题1: Baseline (alpha=0) 也触发 steering

**原因**：原代码中的 steering 条件仅检查 `risk >= threshold`，即使 `alpha=0` 也会执行 steering 计算。

```python
# 原代码（错误）
if risk_value >= args.risk_threshold:
    steering_strength = args.steer_alpha * risk_value  # 0.0 * 0.99 = 0.0
    # 但后续计算仍然执行，导致 logits 被破坏
```

**修复**：增加 `alpha > 0` 的条件

```python
# 修复后
if args.steer_alpha > 0 and risk_value >= args.risk_threshold:
    # ...
```

**结果**：Baseline 现在能够正常生成，不再出现 NASL。

### 问题2: TSV Steering 导致 logits 数值爆炸

**原因**：TSV 向量在训练时只优化了隐藏空间中的簇分离（使用 Optimal Transport Loss），完全没有考虑：
1. 对 `lm_head` 投影的影响
2. Vocab space 中的分布

当应用 steering 时：

```python
steered_hidden = hidden + alpha * risk * tsv_vector  # alpha=2.0, risk=0.99
steered_logits = matmul(steered_hidden, lm_head.weight.T)
```

**实际发生的事情**：

1. TSV 向量范数：6.94
2. 偏移量：`2.0 * 0.99 * 6.94 = 13.75` （约 30% 的 hidden state 范数）
3. Steered hidden 范数：从 45 暴涨到 **728.5**！
4. NASL token (ID 49151) 的 logit：
   - 原始：-16.8
   - Steered：**+472.5** （增加了 489！）
5. 结果：softmax 后 NASL 概率变成 **100.0%**，其他所有 token 概率为 **0%**

**分析**：TSV 向量的某个方向与 `lm_head.weight[49151]`（NASL token）高度对齐，导致微小的偏移产生巨大的 logit 变化。

## 尝试的修复方案

### 方案 A: 限制偏移量 (已实现，但不完全有效)

```python
max_offset_ratio = 0.2  # 最多偏移 20% 的 hidden state 范数
max_offset = hidden_norm * max_offset_ratio
actual_offset = min(steering_strength * tsv_vector.norm(), max_offset)
```

**结果**：即使限制到 20%，NASL 仍然爆炸。

### 方案 B: 归一化 TSV 向量 (未完全实现)

```python
tsv_normalized = tsv_vector / tsv_vector.norm()
steered_hidden = hidden + alpha * risk * tsv_normalized * scale
```

**问题**：需要调整 `scale` 参数，且不保证不会触发其他 token 的爆炸。

### 方案 C: 重新训练 TSV (推荐)

**思路**：在 TSV 训练时，增加对 lm_head 投影的约束：

```python
# 训练时
tsv_hidden = hidden + lambda * tsv
tsv_logits = matmul(tsv_hidden, lm_head.weight.T)

# 增加 logits 分布的约束
logits_kl_loss = KL(original_logits, tsv_logits)
total_loss = ot_loss + beta * logits_kl_loss
```

**优点**：
- 保证 TSV 在 vocab space 中的行为可控
- 不会产生意外的 token 爆炸

### 方案 D: 修改 Steering 方式 (最简单)

**思路**：不直接操作 hidden state，而是操作 logits

```python
# 计算两个方向的 logits
original_logits = matmul(hidden, lm_head.weight.T)
tsv_direction_logits = matmul(tsv_vector, lm_head.weight.T)

# 在 logits 空间混合
steered_logits = original_logits + alpha * risk * tsv_direction_logits
```

**优点**：
- 简单直接
- logits 的变化是线性的，不会爆炸

## 当前状态

- ✅ **Baseline (alpha=0)**: 正常生成，steering_trigger_rate = 0.0
- ❌ **Steering (alpha>0)**: 仍然产生 NASL 重复

## 推荐解决方案

**短期（实验演示）**：

1. 使用**非常小的 `steer_alpha`**（如 0.01 ~ 0.05），避免数值爆炸
2. 实现**方案 D（logits 空间 steering）**

**长期（正式实验）**：

1. 重新训练 TSV，使用**方案 C**，在训练时加入 lm_head 约束
2. 或者完全改变 steering 策略，使用 **Contrastive Decoding** 或 **Logit Lens** 方法

## 文件清理建议

调试过程中创建的临时文件：
- `debug_generation.py`
- `debug_full_pipeline.py`

可以删除。

